{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**EXP**-**1-  Build CNN model for sample dataset**"
      ],
      "metadata": {
        "id": "aU0jwIjDJ8v8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets -qq\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import opendatasets as od\n",
        "od.download('https://www.kaggle.com/datasets/hemendrasr/pizza-vs-ice-cream')\n",
        "train_dir = r'/content/pizza-vs-ice-cream/dataset/train'\n",
        "test_dir = r'/content/pizza-vs-ice-cream/dataset/test'\n",
        "val_dir = r'/content/pizza-vs-ice-cream/dataset/valid'\n",
        "train_generator = image_dataset_from_directory(train_dir, image_size=(128, 128), batch_size=32)\n",
        "test_generator = image_dataset_from_directory(test_dir, image_size=(128, 128), batch_size=32)\n",
        "val_generator = image_dataset_from_directory(val_dir, image_size=(128, 128), batch_size=32)\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in val_generator:\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")\n",
        "model = Sequential([\n",
        "    Conv2D(64, (3, 3), input_shape=(128, 128, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(512, (3, 3), activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()\n",
        "plt.title('Training Log')\n",
        "plt.plot(logs.history['loss'], label='Training Loss')\n",
        "plt.plot(logs.history['accuracy'], label='Training Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.title('Validation Log')\n",
        "plt.plot(logs.history['val_loss'], label='Validation Loss')\n",
        "plt.plot(logs.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "model.evaluate(test_generator)\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
        "y_true = np.concatenate([y for x, y in test_generator], axis=0)\n",
        "import numpy as np\n",
        "y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
        "y_true = np.concatenate([y for x, y in test_generator], axis=0)\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "print('Classification Report')\n",
        "print(classification_report(y_true, y_pred_classes))"
      ],
      "metadata": {
        "id": "MDOxYizeKAQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXP-2-Implement transfer learning with pretrained CNN model**"
      ],
      "metadata": {
        "id": "X6lkiJbhKKnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq opendatasets\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import opendatasets as od\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "od.download(\"https://www.kaggle.com/datasets/hemendrasr/pizza-vs-ice-cream\")\n",
        "\n",
        "train_dir = '/content/pizza-vs-ice-cream/dataset/train'\n",
        "test_dir = '/content/pizza-vs-ice-cream/dataset/test'\n",
        "\n",
        "from keras.utils import image_dataset_from_directory\n",
        "train_dataset = image_dataset_from_directory(train_dir, image_size=(600, 600), batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(test_dir, image_size=(600, 600), batch_size=32)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in test_dataset:\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(int(labels[i]))\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "input_shape = (600, 600, 3)\n",
        "\n",
        "base_model = tf.keras.applications.VGG16(\n",
        "    input_shape=input_shape,\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "base_model = tf.keras.applications.InceptionV3(\n",
        "    input_shape=input_shape,\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "base_model = tf.keras.applications.ResNet50(\n",
        "    input_shape=input_shape,\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "resnet = models.Sequential([\n",
        "    base_model,\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "resnet.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = resnet.fit(\n",
        "    train_dataset,\n",
        "    validation_data=test_dataset,\n",
        "    epochs=1\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "resnet.evaluate(test_dataset)\n",
        "\n",
        "y_pred = resnet.predict(test_dataset)\n",
        "y_pred_classes = np.where(y_pred > 0.5, 1, 0)\n",
        "# y_pred_classes = np.argmax(y_pred, axis=1)  # For multi-class classification\n",
        "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n",
        "print('Classification Report')\n",
        "print(classification_report(y_true, y_pred_classes))"
      ],
      "metadata": {
        "id": "CJmM4PFkKM8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXP-3-Implement transfer learning for imge classification with CIFAR-10 dataset**"
      ],
      "metadata": {
        "id": "mB3NvyA3KQvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "output = Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=1,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "y_pred = model.predict(test_images)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.concatenate(test_labels, axis=0)\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "# Classification Report\n",
        "print('Classification Report')\n",
        "print(classification_report(y_true, y_pred_classes))"
      ],
      "metadata": {
        "id": "qeIEeDkbKSeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXP-4-Apply transfer learning for dog breed identification dataset**"
      ],
      "metadata": {
        "id": "zEl4z9wKKXX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"hartman/dog-breed-identification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D,Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "data=pd.read_csv(r'C:\\Users\\srujo\\.cache\\kagglehub\\datasets\\hartman\\dog-breed-identification\\versions\\1\\labels.csv')\n",
        "data.head()\n",
        "\n",
        "data['id']=data['id'].apply(lambda x:x+'.jpg')\n",
        "data.head()\n",
        "\n",
        "size=(224,224)\n",
        "\n",
        "datagen=ImageDataGenerator(rescale=1./255,\n",
        "                           rotation_range=20,\n",
        "                           shear_range=0.2,\n",
        "                           zoom_range=0.2,\n",
        "                           validation_split=0.3,\n",
        "                           fill_mode='nearest',\n",
        "                           horizontal_flip=True)\n",
        "\n",
        "train=datagen.flow_from_dataframe(data,directory=r'C:\\Users\\srujo\\.cache\\kagglehub\\datasets\\hartman\\dog-breed-identification\\versions\\1\\train',\n",
        "                                  x_col='id',\n",
        "                                  y_col='breed',\n",
        "                                  class_mode='categorical',\n",
        "                                  target_size=size,\n",
        "                                  batch_size=32,\n",
        "                                  subset='training')\n",
        "\n",
        "test=datagen.flow_from_dataframe(data,directory=r'C:\\Users\\srujo\\.cache\\kagglehub\\datasets\\hartman\\dog-breed-identification\\versions\\1\\train',\n",
        "                                 x_col='id',\n",
        "                                 y_col='breed',\n",
        "                                 class_mode='categorical',\n",
        "                                 target_size=size,\n",
        "                                 batch_size=32,\n",
        "                                 subset='validation')\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "vgg=VGG16(weights='imagenet',include_top=False,input_shape=(64,64,3))\n",
        "vgg.trainable=False\n",
        "model=vgg.output\n",
        "model=GlobalAveragePooling2D()(model)\n",
        "model=Dropout(0.2)(model)\n",
        "model=Dense(512,activation='relu')(model)\n",
        "pred=Dense(120,activation='softmax')(model)\n",
        "mm=tensorflow.keras.models.Model(vgg.input,pred)\n",
        "\n",
        "mm.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "er=EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)\n",
        "his=mm.fit(train,epochs=5,validation_data=test,callbacks=[er])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "img, label = next(train)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i in range(12):\n",
        "    fig.add_subplot(3, 4, i+1)\n",
        "    plt.imshow(img[i])\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "NA9IT804KajI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXP-5-Build review sentiment classifier using transfer learning**"
      ],
      "metadata": {
        "id": "d1DQeujILsRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tabulate import tabulate\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('/content/train.csv')\n",
        "df_test = pd.read_csv('/content/test.csv')\n",
        "\n",
        "df_train.head()\n",
        "\n",
        "df_train.info()\n",
        "\n",
        "print(df_train['1'].value_counts())\n",
        "print(df_test['1'].value_counts())\n",
        "\n",
        "# Preprocess text\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.lower()\n",
        "    text = [word for word in text.split() if word not in stop_words]\n",
        "    return ' '.join(text)\n",
        "\n",
        "df_train['processed_text'] = df_train['0'].apply(preprocess_text)\n",
        "df_test['processed_text'] = df_test['0'].apply(preprocess_text)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "vocab_size = 10000\n",
        "max_len = 200  # maximum length of a sequence\n",
        "embedding_dim = 16\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df_train['processed_text'])\n",
        "\n",
        "train_seq = tokenizer.texts_to_sequences(df_train['processed_text'])\n",
        "test_seq = tokenizer.texts_to_sequences(df_test['processed_text'])\n",
        "\n",
        "train_pad = pad_sequences(train_seq, maxlen=max_len, padding='post')\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "train_label = to_categorical(df_train['1'])\n",
        "test_label = to_categorical(df_test['1'])\n",
        "\n",
        "y_train = df_train['1'].values\n",
        "y_train\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_pad, df_train['1'].values, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Plot training history\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "test_seq_padded = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "test_seq_np = np.array(test_seq_padded)\n",
        "\n",
        "predictions = model.predict(test_seq_np)\n",
        "predicted_labels = np.round(predictions)\n",
        "true_labels = np.array(df_test['1'])\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"size\": 16})\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "table = [\n",
        "    [\"Accuracy\", accuracy],\n",
        "    [\"Precision\", precision],\n",
        "    [\"Recall\", recall],\n",
        "    [\"F1-score\", f1]\n",
        "]\n",
        "\n",
        "print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "eFvr4js2Ly1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXP-6-Apply transfer learning for IMDB dataset with word embeddings**"
      ],
      "metadata": {
        "id": "ZIPx_emZKf1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tabulate import tabulate\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('/content/train.csv')\n",
        "df_test = pd.read_csv('/content/test.csv')\n",
        "\n",
        "df_train.head()\n",
        "\n",
        "df_train.info()\n",
        "\n",
        "print(df_train['1'].value_counts())\n",
        "print(df_test['1'].value_counts())\n",
        "\n",
        "# Preprocess text\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.lower()\n",
        "    text = [word for word in text.split() if word not in stop_words]\n",
        "    return ' '.join(text)\n",
        "\n",
        "df_train['processed_text'] = df_train['0'].apply(preprocess_text)\n",
        "df_test['processed_text'] = df_test['0'].apply(preprocess_text)\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=df_train['processed_text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_model.save(\"word2vec.model\")\n",
        "\n",
        "# Create embedding matrix\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "word_index = {word: i+1 for i, word in enumerate(word2vec_model.wv.index_to_key)}\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size:\n",
        "        try:\n",
        "            embedding_vector = word2vec_model.wv[word]\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embedding_dim)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df_train['processed_text'])\n",
        "\n",
        "train_seq = tokenizer.texts_to_sequences(df_train['processed_text'])\n",
        "test_seq = tokenizer.texts_to_sequences(df_test['processed_text'])\n",
        "\n",
        "average_len = np.mean([len(seq) for seq in train_seq])\n",
        "max_len = int(average_len + 100)\n",
        "\n",
        "train_pad = pad_sequences(train_seq, maxlen=max_len, padding='post')\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "train_label = to_categorical(df_train['1'])\n",
        "test_label = to_categorical(df_test['1'])\n",
        "\n",
        "y_train = df_train['1'].values\n",
        "y_train\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_pad, df_train['1'].values, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Plot training history\n",
        "plt.style.use('dark_background')\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_seq_padded = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "test_seq_np = np.array(test_seq_padded)\n",
        "\n",
        "predictions = model.predict(test_seq_np)\n",
        "predicted_labels = np.round(predictions)\n",
        "true_labels = np.array(df_test['1'])\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"size\": 16})\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "table = [\n",
        "    [\"Accuracy\", accuracy],\n",
        "    [\"Precision\", precision],\n",
        "    [\"Recall\", recall],\n",
        "    [\"F1-score\", f1]\n",
        "]\n",
        "\n",
        "print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "id": "WH2pEb2SMwSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXP-7-Create document summaries using transfer learning**"
      ],
      "metadata": {
        "id": "arxkarD2Kyog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
        "train_data, test_data = imdb['train'], imdb['test']\n",
        "\n",
        "train_sentences = []\n",
        "train_labels = []\n",
        "\n",
        "for s, l in train_data:\n",
        "    train_sentences.append(str(s.numpy()))\n",
        "    train_labels.append(l.numpy())\n",
        "\n",
        "test_sentences = []\n",
        "test_labels = []\n",
        "\n",
        "for s, l in test_data:\n",
        "    test_sentences.append(str(s.numpy()))\n",
        "    test_labels.append(l.numpy())\n",
        "\n",
        "vocab_size = 10000\n",
        "max_length = 200\n",
        "embedding_dim = 16\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "train_labels = np.array(train_labels)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 5\n",
        "model.fit(train_padded, train_labels, epochs=num_epochs, validation_data=(test_padded, test_labels))\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/MyDrive/Temp/imdb_model.h5\")\n",
        "\n",
        "\"\"\"# Use Trained Model To implement TL\"\"\"\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "imdb_model = tf.keras.models.load_model('/content/drive/MyDrive/Temp/imdb_model.h5')\n",
        "\n",
        "for layer in imdb_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "corpus = [\n",
        "    \"The movie had a very strong start.\",\n",
        "    \"However, the plot quickly fell apart.\",\n",
        "    \"The acting was top-notch, especially the lead actor.\",\n",
        "    \"But the storyline was predictable and uninspiring.\",\n",
        "    \"Overall, the movie had good moments but was disappointing.\"\n",
        "]\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(corpus)\n",
        "max_length = 200\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "max_length = 200\n",
        "input_shape = (max_length,)\n",
        "inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "x = imdb_model(inputs)\n",
        "\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "summarization_model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "summarization_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "summarization_model.summary()\n",
        "\n",
        "predictions = summarization_model.predict(padded_sequences)\n",
        "\n",
        "summary = [corpus[i] for i, score in enumerate(predictions) if score > 0.436]\n",
        "\n",
        "print(\"Summary:\")\n",
        "for sentence in summary:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "qfUjJA-sK1xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXP-8-Perform Audio event classification with transfer learning**"
      ],
      "metadata": {
        "id": "QTgpzQagK5tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df= pd.read_csv('/content/urbansound8k/data.csv')\n",
        "df.head(5)\n",
        "\n",
        "import librosa\n",
        "audio_file_path='/content/urbansound8k/fold10/100648-1-0-0.wav'\n",
        "librosa_audio_data,librosa_sample_rate=librosa.load(audio_file_path)\n",
        "\n",
        "print(librosa_audio_data)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(librosa_audio_data)\n",
        "\n",
        "# @title Import Modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os, librosa\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from keras.utils import to_categorical\n",
        "from keras import layers, Sequential\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from keras.layers import Conv2D, MaxPooling2D,BatchNormalization\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "mfccs = librosa.feature.mfcc(y=librosa_audio_data, sr=librosa_sample_rate, n_mfcc=40)\n",
        "\n",
        "mfccs\n",
        "\n",
        "# @title DataPath\n",
        "audio_dataset_path = '/content/data'\n",
        "metadata =  pd.read_csv('/content/urbansound8k/data.csv')\n",
        "metadata.head()\n",
        "\n",
        "def mfccExtract(file):\n",
        "    waveform, sampleRate = librosa.load(file_name)\n",
        "    features = librosa.feature.mfcc(y = waveform, sr = sampleRate, n_mfcc = 50)\n",
        "    return np.mean(features, axis = 1)\n",
        "\n",
        "extractAll = []\n",
        "\n",
        "for index_num, row in tqdm(metadata.iterrows()):\n",
        "    # Constructing file path\n",
        "    file_name = os.path.join(audio_dataset_path, 'fold' + str(row['fold']), row['slice_file_name'])\n",
        "\n",
        "    # Extracting features and appending them\n",
        "    features = mfccExtract(file_name)\n",
        "    extractAll.append([features, row['class']])\n",
        "\n",
        "featuresDf = pd.DataFrame(extractAll, columns = ['Features', 'Class'])\n",
        "featuresDf.head()\n",
        "\n",
        "X=np.array(featuresDf['Features'].tolist())\n",
        "Y=np.array(featuresDf['Class'].tolist())\n",
        "\n",
        "labelencoder=LabelEncoder()\n",
        "Y=to_categorical(labelencoder.fit_transform(Y))\n",
        "\n",
        "Y\n",
        "\n",
        "### Train Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)\n",
        "num_labels=Y.shape[1]\n",
        "\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from keras.layers import *\n",
        "base_model = VGG19(weights='imagenet',\n",
        "                       include_top=False,\n",
        "                       input_shape=(32, 32, 3))\n",
        "\n",
        "from tensorflow.keras.models import  Model\n",
        "# Add classification layers on top of it\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, activation='relu',input_shape = (50,))(x)\n",
        "output = Dense(10, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
        "\n",
        "history = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs = 10)\n",
        "\n",
        "test_accuracy=model.evaluate(X_test,Y_test,verbose=0)\n",
        "print(test_accuracy[1])\n",
        "\n",
        "historyDf = pd.DataFrame(history.history)\n",
        "\n",
        "historyDf.loc[:, ['loss', 'val_loss']].plot()\n",
        "\n",
        "historyDf.loc[:, ['accuracy', 'val_accuracy']].plot()\n",
        "\n",
        "# Evaluating model\n",
        "score = model.evaluate(X_test, Y_test)[1] * 100\n",
        "print(f'Validation accuracy of model : {score:.2f}%')"
      ],
      "metadata": {
        "id": "jDyAKfInK8CS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
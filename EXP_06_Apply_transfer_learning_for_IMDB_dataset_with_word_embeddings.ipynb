{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txYm-mvfITnw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tabulate import tabulate\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv('/content/train.csv')\n",
        "df_test = pd.read_csv('/content/test.csv')\n",
        "\n",
        "df_train.head()\n",
        "\n",
        "df_train.info()\n",
        "\n",
        "print(df_train['1'].value_counts())\n",
        "print(df_test['1'].value_counts())\n",
        "\n",
        "# Preprocess text\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = text.lower()\n",
        "    text = [word for word in text.split() if word not in stop_words]\n",
        "    return ' '.join(text)\n",
        "\n",
        "df_train['processed_text'] = df_train['0'].apply(preprocess_text)\n",
        "df_test['processed_text'] = df_test['0'].apply(preprocess_text)\n",
        "\n",
        "# Train Word2Vec model\n",
        "word2vec_model = Word2Vec(sentences=df_train['processed_text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "word2vec_model.save(\"word2vec.model\")\n",
        "\n",
        "# Create embedding matrix\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "word_index = {word: i+1 for i, word in enumerate(word2vec_model.wv.index_to_key)}\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < vocab_size:\n",
        "        try:\n",
        "            embedding_vector = word2vec_model.wv[word]\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            embedding_matrix[i] = np.random.normal(0, np.sqrt(0.25), embedding_dim)\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df_train['processed_text'])\n",
        "\n",
        "train_seq = tokenizer.texts_to_sequences(df_train['processed_text'])\n",
        "test_seq = tokenizer.texts_to_sequences(df_test['processed_text'])\n",
        "\n",
        "average_len = np.mean([len(seq) for seq in train_seq])\n",
        "max_len = int(average_len + 100)\n",
        "\n",
        "train_pad = pad_sequences(train_seq, maxlen=max_len, padding='post')\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "train_label = to_categorical(df_train['1'])\n",
        "test_label = to_categorical(df_test['1'])\n",
        "\n",
        "y_train = df_train['1'].values\n",
        "y_train\n",
        "\n",
        "# Build the model\n",
        "model = tf.keras.models.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_len, trainable=False),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_pad, df_train['1'].values, epochs=10, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# Plot training history\n",
        "plt.style.use('dark_background')\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "plt.style.use('dark_background')\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "test_seq_padded = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "test_seq_np = np.array(test_seq_padded)\n",
        "\n",
        "predictions = model.predict(test_seq_np)\n",
        "predicted_labels = np.round(predictions)\n",
        "true_labels = np.array(df_test['1'])\n",
        "\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", annot_kws={\"size\": 16})\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "f1 = f1_score(true_labels, predicted_labels)\n",
        "\n",
        "table = [\n",
        "    [\"Accuracy\", accuracy],\n",
        "    [\"Precision\", precision],\n",
        "    [\"Recall\", recall],\n",
        "    [\"F1-score\", f1]\n",
        "]\n",
        "\n",
        "print(tabulate(table, headers=[\"Metric\", \"Value\"], tablefmt=\"fancy_grid\"))"
      ]
    }
  ]
}